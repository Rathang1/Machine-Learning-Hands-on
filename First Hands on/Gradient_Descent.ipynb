{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Algorithm\n",
        "\n",
        "Adapted from https://realpython.com/gradient-descent-algorithm-python/\n",
        "\n",
        "## Minimizing Convex Functions\n",
        "\n",
        "Convex functions have at least one minima. Examples of convex functions are:\n",
        "\n",
        "$ y = x^2$ \n",
        "\n",
        "$y = (x-2)^2$\n",
        "\n",
        "$y = (x-2) * (x-3)$\n",
        "\n",
        "How do we find the minima of such a function. Two possible ways:\n",
        "\n",
        "1. Analytical Technique: Use standard calculus. \n",
        "\n",
        "2. Numerical Technique: Use an algorithm such as *gradient descent* that can be programmed.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVz-JbxFJXOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPV0taUd97p1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " \n",
        "def gradient_descent(\n",
        "     gradient, start, learn_rate, n_iter=50, tolerance=1e-06\n",
        " ):\n",
        "  vector = start\n",
        "  for _ in range(n_iter):\n",
        "    diff = -learn_rate * gradient(vector)\n",
        "    if np.all(np.abs(diff) <= tolerance):\n",
        "      break\n",
        "    vector += diff\n",
        "  return vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(\n",
        "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "InS7DY7m-gtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478737cb-cb64-4d2f-dd1b-6ac9182e710b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.210739197207331e-06"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(\n",
        "    gradient=lambda v: np.array([2 * v[0], 4 * v[1]**3]),\n",
        "    start=np.array([1.0, 1.0]), learn_rate=0.2, tolerance=1e-08\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3wVNuHdsUfr",
        "outputId": "6633ae22-3e22-4ca3-d5d7-a3195f7a88f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.08281277e-12, 9.75207120e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(\n",
        "    gradient=lambda v: 1 - 1 / v, start=2.5, learn_rate=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faobPh7B-FJY",
        "outputId": "fe449e46-2700-4e56-834a-c5a9e062016c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000011077232125"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Function \n",
        "\n",
        "The function that we seek to minimize is the mean squared error, which is defined as the square of the difference between actual and predicted values. This is also referred to as the cost function. Let's call it $J$\n",
        "\n",
        "$J = \\displaystyle \\frac{1}{2n} \\sum_{i=1}^{n}(\\text{Predicted Value} - \\text{Actual Value})^2$\n",
        "\n",
        "where the sum is over all $n$ data points. \n",
        "\n",
        "Suppose the predicted model is $y = w_0 + w_1 x_1$ where $w_0$ and $w_1$ are weights or coefficients for the bias and the first variable $x_1$ and let actual value be $y_a$\n",
        "\n",
        "$J = \\displaystyle \\frac{1}{2n} \\sum_{i=1}^{n} [(w_0 + w_1 x_1) - y_a]^2$\n",
        "\n",
        "To find the gradient w.r.t each of the w variables, we do the following:\n",
        "\n",
        "$\\displaystyle\\frac{\\partial J}{\\partial w_0} = \\displaystyle \\frac{1}{n} \\sum_{i=1}^{n}  [(w_0 + w_1 x_1) - y_a]$  \n",
        "\n",
        "$\\displaystyle\\frac{\\partial J}{\\partial w_1} = \\displaystyle \\frac{1}{n} \\sum_{i=1}^{n} x_1 [(w_0 + w_1 x_1) - y_a]$\n",
        "\n",
        "We can use above gradient functions to minimize the convex error function.\n",
        "\n",
        "The value $[(w_0 + w_1 x_1) - y_a]$ is called the residual i.e. difference between actual and predicted value for a data point.\n"
      ],
      "metadata": {
        "id": "rjdYPv8GCJ90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ssr_gradient(x, y, w):\n",
        "    res = w[0] + w[1] * x - y\n",
        "    return res.mean(), (res * x).mean()  # .mean() is a method of np.ndarray"
      ],
      "metadata": {
        "id": "znfxYtROWuBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent(\n",
        "     gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06\n",
        " ):\n",
        "  vector = start\n",
        "  for _ in range(n_iter):\n",
        "    diff = -learn_rate * np.array(gradient(x, y, vector))\n",
        "    if np.all(np.abs(diff) <= tolerance):\n",
        "      break\n",
        "    vector += diff\n",
        "  return vector"
      ],
      "metadata": {
        "id": "6BGWd-UeW4CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([5, 15, 25, 35, 45, 55])\n",
        "y = np.array([5, 20, 14, 32, 22, 38])\n",
        "\n",
        "gradient_descent(\n",
        "    ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,\n",
        "    n_iter=100_000\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4zgakrBaY36",
        "outputId": "d40a96f9-c234-4c89-9dc9-787e511afa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.62822349, 0.54012867])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please see the following links for more details:\n",
        "\n",
        "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
        "\n",
        "Code for above: https://colab.research.google.com/gist/sagarmainkar/5cfa33898a303f895da5100472371d91/notebook.ipynb#scrollTo=yOs3ormtY9al\n",
        "\n",
        "For a vectorized approach, see\n",
        "\n",
        "https://www.geeksforgeeks.org/vectorization-of-gradient-descent/\n",
        "\n",
        "and\n",
        "\n",
        "https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645\n",
        "\n"
      ],
      "metadata": {
        "id": "ICY_7Fg6cKxn"
      }
    }
  ]
}